{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "984b565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import pickle\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, brier_score_loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edafeb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "gamma = 1\n",
    "batch_size = 256\n",
    "seq_len = 1\n",
    "learning_rate = 1e-4\n",
    "\n",
    "max_epoch = 100\n",
    "experiment_time = 5\n",
    "limit_early_stop_count = 5\n",
    "\n",
    "show_shap_flag = True\n",
    "select_feature_flag = False\n",
    "use_upsample = False\n",
    "use_mini_feature = False\n",
    "only_Weaning = False\n",
    "\n",
    "task_name_list = ['Weaning_successful']\n",
    "\n",
    "#data_date = \"20240104\"\n",
    "data_date = \"20240114\"\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "786bc782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_MTL(nn.Module):\n",
    "    def __init__(self, input_dim, task_name_list, dropout_ratio=0.0):\n",
    "        super(MLP_MTL, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.relu = nn.ReLU()  # Activation function for hidden layers\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.task_name_list = task_name_list\n",
    "        self.num_tasks = len(task_name_list)\n",
    "        hidden_dim = [256, 128, 64, 32]\n",
    "        output_size = 1\n",
    "\n",
    "        # Bottom\n",
    "        self.bt_fc1 = nn.Linear(input_dim, hidden_dim[0])\n",
    "        self.bt_fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
    "        self.bt_fc3 = nn.Linear(hidden_dim[1], hidden_dim[2])\n",
    "\n",
    "        # Towers\n",
    "        self.task_fc0 = nn.ModuleList([nn.Linear(hidden_dim[2], hidden_dim[3]) for _ in range(self.num_tasks)])\n",
    "        self.task_fc1 = nn.ModuleList([nn.Linear(hidden_dim[3], output_size) for _ in range(self.num_tasks)])\n",
    "    \n",
    "    def data_check(self,x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "        if x.ndim == 3:\n",
    "            x = x.reshape(x.shape[0], x.shape[1] * x.shape[2])  # Flatten \n",
    "            \n",
    "        x = x.to(device)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.data_check(x)\n",
    "\n",
    "        # Bottom\n",
    "        x = self.bt_fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bt_fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bt_fc3(x)\n",
    "        h = self.relu(x)\n",
    "        h = self.dropout(h)  \n",
    "\n",
    "        # Towers\n",
    "        task_out = {}\n",
    "        for task_index in range(self.num_tasks):\n",
    "            task_name = self.task_name_list[task_index]\n",
    "            hi = self.task_fc0[task_index](h)\n",
    "            hi = self.relu(hi)\n",
    "            hi = self.dropout(hi)\n",
    "            hi = self.task_fc1[task_index](hi)\n",
    "            hi = self.sigmoid(hi)\n",
    "            task_out[task_name] = hi    \n",
    "            \n",
    "        if len(self.task_name_list) == 1:\n",
    "            return task_out[self.task_name_list[0]]\n",
    "        else:\n",
    "            return task_out\n",
    "    \n",
    "    def predict_prob(self, x):\n",
    "        self.eval()\n",
    "        prob_dict = self.forward(x)\n",
    "        \n",
    "        if len(self.task_name_list) == 1:\n",
    "            prob_dict_true = {}\n",
    "            prob_dict_true[self.task_name_list[0]] = prob_dict\n",
    "            return prob_dict_true\n",
    "        return prob_dict\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        self.eval()\n",
    "        prob_dict = self.forward(x)\n",
    "        \n",
    "        if len(self.task_name_list) == 1:\n",
    "            prob_dict_true = {}\n",
    "            prob_dict_true[self.task_name_list[0]] = prob_dict\n",
    "            return prob_dict_true\n",
    "        \n",
    "        return prob_dict\n",
    "    \n",
    "    def predict(self, x, threshold = 0.5):\n",
    "        self.eval()\n",
    "        prob_dict = self.predict_prob(x)\n",
    "        pred_dict = {}\n",
    "        \n",
    "        for key, value in prob_dict.items():\n",
    "            #tensor轉numpy\n",
    "            value = value.cpu().detach().numpy()\n",
    "            pred_class = [1 if x > threshold else 0 for x in value]\n",
    "            pred_dict[key] = np.array(pred_class) \n",
    "        return pred_dict\n",
    "    \n",
    "    def evaluate(self,X,label,task_name,criterion):\n",
    "        with torch.no_grad():\n",
    "            prob = self.predict_prob(X)[task_name].cpu().detach().numpy() #tensor=>numpy\n",
    "            pred = self.predict(X)[task_name] \n",
    "            score = compute_scores(label,pred,prob)\n",
    "            score['task'] = task_name\n",
    "            loss = criterion(torch.from_numpy(prob).to(device),torch.from_numpy(label).to(device)).item()\n",
    "            score['loss'] = loss/len(label)\n",
    "            return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8537c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(y_true, y_pred,y_prob):\n",
    "    if np.any(np.isnan(y_prob)):\n",
    "        print(y_prob)\n",
    "        input()\n",
    "        \n",
    "    scores = {}\n",
    "    try:\n",
    "        scores['task'] = 'Null'\n",
    "        scores['auroc'] = round(roc_auc_score(y_true, y_prob), 3)\n",
    "        scores['acc'] = round(accuracy_score(y_true, y_pred), 3)\n",
    "        scores['f1'] = round(f1_score(y_true, y_pred), 3)\n",
    "        scores['pre'] = round(precision_score(y_true, y_pred), 3)\n",
    "        scores['recall'] = round(recall_score(y_true, y_pred), 3)\n",
    "        scores['brier_score'] = round(brier_score_loss(y_true, y_prob), 3)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8daed213",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "    model\n",
    "    dict: Mydataset\n",
    "    loss_function\n",
    "Output:\n",
    "    score: dict + dict\n",
    "    result: dict => ['total_auc','total_loss']\n",
    "\"\"\"\n",
    "def test(model, dataset_dict, criterion, is_show = True , only_Weaning = False):\n",
    "    model.eval()\n",
    "\n",
    "    task_name_list = list(dataset_dict.keys())\n",
    "    score = {}\n",
    "    result = {'total_auc': 0, 'total_loss': 0}\n",
    "    for task_name in task_name_list:  # 循環每個任務\n",
    "        X = dataset_dict[task_name].inputs.numpy()\n",
    "        Y = dataset_dict[task_name].labels.unsqueeze(1).numpy()\n",
    "    \n",
    "        score[task_name] = model.evaluate(X,Y,task_name,criterion)\n",
    "        \n",
    "        if only_Weaning == True and 'Weaning_succecssful' in task_name_list:\n",
    "            if task_name == 'Weaning_succecssful':\n",
    "                result['total_auc'] = result['total_auc'] + score[task_name]['auroc']\n",
    "                result['total_loss'] = result['total_loss'] + score[task_name]['loss']\n",
    "        else:\n",
    "            result['total_auc'] = result['total_auc'] + score[task_name]['auroc']\n",
    "            result['total_loss'] = result['total_loss'] + score[task_name]['loss']\n",
    "            \n",
    "        if is_show:\n",
    "            print(score[task_name])\n",
    "    \n",
    "    return score,result\n",
    "\n",
    "\"\"\"\n",
    "local_best_model_dict: #dict{'task_name':{'model','performance(target_score)','id'}}\n",
    "model\n",
    "\"\"\"\n",
    "def test2(local_best_model_dict, modelr, dataset_dict, criterion, is_show = True):\n",
    "    score = {}\n",
    "    result = {'total_auc': 0, 'total_loss': 0}\n",
    "    task_name_list = list(dataset_dict.keys())\n",
    "    \n",
    "    for task_name in task_name_list:\n",
    "        print(f\"task: {task_name} \")\n",
    "        print(f\"{local_best_model_dict[task_name]['performance']}\")\n",
    "        modelr.load_state_dict(local_best_model_dict[task_name]['model'])\n",
    "        modelr.eval()\n",
    "        X = dataset_dict[task_name].inputs.numpy()\n",
    "        Y = dataset_dict[task_name].labels.unsqueeze(1).numpy()\n",
    "        score[task_name] = modelr.evaluate(X,Y,task_name,criterion)\n",
    "        result['total_auc'] = result['total_auc'] + score[task_name]['auroc']\n",
    "        result['total_loss'] = result['total_loss'] + score[task_name]['loss']\n",
    "        if is_show:\n",
    "            print(score[task_name])\n",
    "            \n",
    "    return score,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d074fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, np_X_scalar,np_X_original, np_Y):\n",
    "        self.inputs = torch.from_numpy(np_X_scalar).float()\n",
    "        self.inputs_original = torch.from_numpy(np_X_original).float()\n",
    "        self.labels = torch.from_numpy(np_Y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "    \n",
    "    def remove_samples(self, feature_index, threshold, condition_type):\n",
    "        \"\"\"\n",
    "        Remove samples based on a specified condition on a specific feature.\n",
    "\n",
    "        Parameters:\n",
    "        - feature_index (int): Index of the feature.\n",
    "        - threshold (float): Threshold value for the condition.\n",
    "        - condition_type (str): Type of condition ('type1' for '<' or 'type2' for '>=').\n",
    "        \"\"\"\n",
    "        if condition_type == 'type1':\n",
    "            indices_to_remove = torch.nonzero(self.inputs[:, feature_index] < threshold).squeeze()\n",
    "        elif condition_type == 'type2':\n",
    "            indices_to_remove = torch.nonzero(self.inputs[:, feature_index] >= threshold).squeeze()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid condition_type. Use 'type1' for '<' or 'type2' for '>='.\")\n",
    "\n",
    "        # Remove samples\n",
    "        self.inputs = torch.index_select(self.inputs, 0, indices_to_remove)\n",
    "        self.inputs_original = torch.index_select(self.inputs_original, 0, indices_to_remove)\n",
    "        self.labels = torch.index_select(self.labels, 0, indices_to_remove)\n",
    "    \n",
    "class BCEFocalLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=2, alpha=0.25, reduction='elementwise_mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    " \n",
    "    def forward(self, _input, target):\n",
    "        pt = _input\n",
    "        alpha = self.alpha\n",
    "        loss = - alpha * (1 - pt) ** self.gamma * target * torch.log(pt) - \\\n",
    "               (1 - alpha) * pt ** self.gamma * (1 - target) * torch.log(1 - pt)\n",
    "        if self.reduction == 'elementwise_mean':\n",
    "            loss = torch.mean(loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = torch.sum(loss)\n",
    "        return loss    \n",
    "\n",
    "    \n",
    "def check_label_distribution (data_Y):\n",
    "    count_1 = np.count_nonzero(data_Y == 1)\n",
    "    count_0 = np.count_nonzero(data_Y == 0)\n",
    "    count_others = np.count_nonzero((data_Y != 1) & (data_Y != 0))\n",
    "    ratio_1 = round(count_1/len(data_Y)*100,2)\n",
    "    ratio_0 = round(count_0/len(data_Y)*100,2)\n",
    "    ratio_others = round(count_others/len(data_Y)*100,2)\n",
    "    print(f'Distribution: 1=>{count_1}({ratio_1}%),  0=>{count_0}({ratio_0}%),  others=>{count_others}({ratio_others}%)')\n",
    "\n",
    "    \n",
    "def upsampling_auto(X,X_original,Y,up_ratio):\n",
    "    check_label_distribution(Y)\n",
    "    zero_idx = np.where(Y == 0)[0]\n",
    "    one_idx = np.where(Y == 1)[0]\n",
    "    other_idx = np.where((Y != 1) & (Y != 0))[0]\n",
    "    if len(other_idx > 0):\n",
    "        return X,Y\n",
    "    repeated_data_X = np.tile(X[one_idx], (up_ratio, 1, 1))\n",
    "    repeated_data_X_original = np.tile(X_original[one_idx], (up_ratio, 1, 1))\n",
    "    repeated_data_Y = np.tile(Y[one_idx], (up_ratio))\n",
    "\n",
    "    X_upsampled = np.vstack((X[zero_idx], repeated_data_X))\n",
    "    X_original_upsampled = np.vstack((X_original[zero_idx], repeated_data_X_original))\n",
    "\n",
    "    Y_upsampled = np.concatenate((Y[zero_idx], repeated_data_Y)) \n",
    "    return X_upsampled,X_original_upsampled, Y_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632dde32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2ad79cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    X: numpy\n",
    "    feature_name_list : List\n",
    "    select_feature_list : List   (必須是feature_name_list的子集)\n",
    "Output\n",
    "    select_feature_list data\n",
    "\"\"\"\n",
    "def select_features(X, feature_name_list, select_feature_list):\n",
    "    invalid_features = set(select_feature_list) - set(feature_name_list)\n",
    "    if invalid_features:\n",
    "        raise ValueError(f\"Invalid features in select_feature_list: {invalid_features}\")\n",
    "    selected_feature_indices = [feature_name_list.index(feature) for feature in select_feature_list]\n",
    "    X_selected = X[:, :, selected_feature_indices]\n",
    "\n",
    "    return X_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f70a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "958621ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_data(task_name_list,data_date,data_type, select_feature_list = [], batch_size = 256,use_upsample = False):\n",
    "    batch_size = 256\n",
    "    data_path = \"data/sample/standard_data\"\n",
    "    \n",
    "    #Feature name\n",
    "    df_feature = pd.read_csv(\"data/sample/full_feature_name.csv\")\n",
    "    feature_name_list = df_feature.columns.to_list()\n",
    "\n",
    "   \n",
    "    #dataset\n",
    "    dataset_dict = {}\n",
    "    original_data_dict = {}\n",
    "    for task_name in task_name_list:\n",
    "        \"\"\"拿掉日期\"\"\"\n",
    "        X_scalar = np.load(f\"{data_path}/{data_type}_scalar_X_{task_name}.npy\", allow_pickle=True)\n",
    "        X_original = np.load(f\"{data_path}/{data_type}_X_{task_name}.npy\", allow_pickle=True)\n",
    "        X_original_with_id = np.load(f\"{data_path}/{data_type}_X_with_id_{task_name}.npy\", allow_pickle=True)\n",
    "        \n",
    "        if len(select_feature_list)>0:\n",
    "            X_scalar = select_features(X_scalar,feature_name_list,select_feature_list)\n",
    "            X_original = select_features(X_original,feature_name_list,select_feature_list)\n",
    "            feature_name_list = select_feature_list\n",
    "    \n",
    "            assert X_scalar.shape[2] == len(select_feature_list)\n",
    "            assert X_original.shape[2] == len(select_feature_list)\n",
    "        X_original_with_id = X_original_with_id[:,:,:1]    \n",
    "        Y = np.load(f\"{data_path}/20240129_{data_type}_Y_{task_name}.npy\", allow_pickle=True)\n",
    "        \n",
    "        if use_upsample:\n",
    "            if task_name == 'Weaning_successful' and data_type == 'test':\n",
    "                X_scalar,X_original,Y = upsampling_auto(X_scalar,X_original,Y,2)\n",
    "        dataset_dict[task_name] = MyDataset(X_scalar,X_original,Y)\n",
    "        original_data_dict['X_scalar'] = X_scalar\n",
    "        original_data_dict['X'] = X_original\n",
    "        original_data_dict['X_with_id'] = X_original_with_id\n",
    "        original_data_dict['Y'] = Y\n",
    "    \n",
    "    #dataloader\n",
    "    loader_dict = {}\n",
    "    for key, dataset in dataset_dict.items():        \n",
    "        loader_dict[key] = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return dataset_dict,loader_dict,feature_name_list,original_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bff3ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "491b737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from datetime import datetime\n",
    "\n",
    "def group_result(df):\n",
    "    agg_columns = {\n",
    "        'acc': ['mean', 'std'],\n",
    "        'pre': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'auroc': ['mean', 'std'],\n",
    "        'brier_score': ['mean', 'std']\n",
    "    }\n",
    "    df_group = df.groupby('task').agg(agg_columns)\n",
    "    df_group.columns = [f\"{col[0]}_{col[1]}\" for col in df_group.columns]\n",
    "\n",
    "    for metric in ['acc', 'pre', 'f1', 'recall', 'auroc','brier_score']:\n",
    "        df_group[f\"{metric}_combined\"] = df_group.apply(\n",
    "            lambda row: f\"{row[f'{metric}_mean']:.4f} ± {row[f'{metric}_std']:.4f}\", axis=1\n",
    "        )\n",
    "\n",
    "    df_result = df_group[[f\"{metric}_combined\" for metric in ['acc', 'pre', 'f1', 'recall', 'auroc','brier_score']]]\n",
    "\n",
    "    df_result.reset_index(inplace=True)\n",
    "    df_result.columns = ['task','acc', 'pre', 'f1', 'recall', 'auroc','brier_score']\n",
    "    return df_result\n",
    "\n",
    "\n",
    "def save_to_xlsx(df_save,file_name = 'output'):\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    for r_idx, row in enumerate(dataframe_to_rows(df_save, index=False, header=True), 1):\n",
    "        for c_idx, value in enumerate(row, 1):\n",
    "            ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "    wb.save(f'{file_name}.xlsx')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e31150f",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edb78051",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'Weaning_successful'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b237e1",
   "metadata": {},
   "source": [
    "# Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31211395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特徵數: 18\n"
     ]
    }
   ],
   "source": [
    "path = \"./model/group_result/mtl_group/vent_group\"\n",
    "df_feature = pd.read_csv(f\"{path}/feature_name_list.csv\")\n",
    "select_feature_list = df_feature['Feature'].tolist()\n",
    "input_dim = len(select_feature_list)\n",
    "loss_func = BCEFocalLoss(alpha=alpha, gamma=gamma)\n",
    "\n",
    "\n",
    "train_dataset_dict,train_loader_dict,feature_name_list,_ = read_data([task_name],\"\",'train',select_feature_list,batch_size = batch_size,use_upsample = use_upsample)\n",
    "val_dataset_dict,val_loader_dict,_ ,_= read_data([task_name],\"\",'validation',select_feature_list,batch_size = batch_size,use_upsample = use_upsample)\n",
    "test_dataset_dict,test_loader_dict,_ ,_= read_data([task_name],\"\",'test',select_feature_list,batch_size = batch_size,use_upsample = use_upsample)\n",
    "\n",
    "print(f'特徵數: {input_dim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdda402",
   "metadata": {},
   "source": [
    "# MTL_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11091438",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = 5  #實驗次數(人工輸入，有多少紀錄就填多少)\n",
    "mode = 'lite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4db5648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]...stl\n",
      "[2]...mtl - vent_group\n",
      "[3]...mtl - mortality_group\n",
      "2\n",
      "==> mtl - vent_group\n"
     ]
    }
   ],
   "source": [
    "group_list = ['stl','mtl - vent_group','mtl - mortality_group']\n",
    "\n",
    "for i in range(0,len(group_list)):\n",
    "    print(f'[{i+1}]...{group_list[i]}')\n",
    "\n",
    "select_mode = int(input())-1\n",
    "print(f'==> {group_list[select_mode]}')\n",
    "\n",
    "\n",
    "if select_mode == 0:\n",
    "    data_path = \"./model/group_result/stl_group\"\n",
    "elif select_mode == 1:\n",
    "    data_path = \"./model/group_result/mtl_group/vent_group\"\n",
    "else:\n",
    "    data_path = \"./model/group_result/mtl_group/mortality_group\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "571ac22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 task              acc              pre               f1  \\\n",
      "0  Weaning_successful  0.7546 ± 0.0042  0.6612 ± 0.0103  0.5822 ± 0.0246   \n",
      "\n",
      "            recall            auroc      brier_score  \n",
      "0  0.5216 ± 0.0435  0.8202 ± 0.0019  0.1656 ± 0.0009  \n"
     ]
    }
   ],
   "source": [
    "\"\"\" Weaning_successful (Vent_group) \"\"\"\n",
    "\n",
    "row_list = []\n",
    "for time in range(1,max_time+1):\n",
    "    model_vent = MLP_MTL(input_dim, task_name_list).to(device)\n",
    "    model_vent.load_state_dict(torch.load(f'{data_path}/{task_name}_{time}_{mode}'))\n",
    "    result,_ = test(model_vent, test_dataset_dict, loss_func, is_show = False)\n",
    "    row_list.append(result[task_name])\n",
    "    #auroc_list.append(result[task_name]['auroc'])\n",
    "    \n",
    "df_result = pd.DataFrame(row_list)    \n",
    "df_result = df_result.sort_values(by = 'auroc', ascending=False)\n",
    "#print(df_result[:5].mean())\n",
    "\n",
    "df_result_group = group_result(df_result[:5])\n",
    "print(df_result_group)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c10194b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Weaning_successful': {'task': 'Weaning_successful', 'auroc': 0.821, 'acc': 0.758, 'f1': 0.598, 'pre': 0.66, 'recall': 0.546, 'brier_score': 0.165, 'loss': 6.205740544593265e-05}}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" best model \"\"\"\n",
    "model_vent = MLP_MTL(input_dim, task_name_list).to(device)\n",
    "model_vent.load_state_dict(torch.load(f'{data_path}/{task_name}_best_{mode}'))\n",
    "result,_ = test(model_vent, test_dataset_dict, loss_func, is_show = False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe74f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca90e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
