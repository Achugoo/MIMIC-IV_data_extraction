{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "984b565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import pickle\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, brier_score_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "edafeb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "gamma = 1\n",
    "seq_len = 1\n",
    "learning_rate = 1e-4\n",
    "\n",
    "batch_size = 32\n",
    "max_epoch = 100\n",
    "experiment_time = 3\n",
    "\n",
    "limit_early_stop_count = 10\n",
    "\n",
    "select_feature_flag = False\n",
    "calculate_shap = True\n",
    "\n",
    "if select_feature_flag:\n",
    "    calculate_shap = True\n",
    "\n",
    "use_upsample = False\n",
    "use_mini_feature = True\n",
    "\n",
    "only_Weaning = False\n",
    "\n",
    "task_name_list = ['Weaning_successful','SBT_start','SBT_successful']\n",
    "#task_name_list = ['Weaning_successful','Mortality_30d','Vasopressor']\n",
    "#task_name_list = ['Weaning_successful']\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "786bc782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_MTL(nn.Module):\n",
    "    def __init__(self, input_dim, task_name_list, dropout_ratio=0.0):\n",
    "        super(MLP_MTL, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.relu = nn.ReLU()  # Activation function for hidden layers\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.task_name_list = task_name_list\n",
    "        self.num_tasks = len(task_name_list)\n",
    "        hidden_dim = [256, 128, 64, 32]\n",
    "        output_size = 1\n",
    "\n",
    "        # Bottom\n",
    "        self.bt_fc1 = nn.Linear(input_dim, hidden_dim[0])\n",
    "        self.bt_fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
    "        self.bt_fc3 = nn.Linear(hidden_dim[1], hidden_dim[2])\n",
    "\n",
    "        # Towers\n",
    "        self.task_fc0 = nn.ModuleList([nn.Linear(hidden_dim[2], hidden_dim[3]) for _ in range(self.num_tasks)])\n",
    "        self.task_fc1 = nn.ModuleList([nn.Linear(hidden_dim[3], output_size) for _ in range(self.num_tasks)])\n",
    "    \n",
    "    def data_check(self,x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "        if x.ndim == 3:\n",
    "            x = x.reshape(x.shape[0], x.shape[1] * x.shape[2])  # Flatten \n",
    "            \n",
    "        x = x.to(device)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.data_check(x)\n",
    "\n",
    "        # Bottom\n",
    "        x = self.bt_fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bt_fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bt_fc3(x)\n",
    "        h = self.relu(x)\n",
    "        h = self.dropout(h)  \n",
    "\n",
    "        # Towers\n",
    "        task_out = {}\n",
    "        for task_index in range(self.num_tasks):\n",
    "            task_name = self.task_name_list[task_index]\n",
    "            hi = self.task_fc0[task_index](h)\n",
    "            hi = self.relu(hi)\n",
    "            hi = self.dropout(hi)\n",
    "            hi = self.task_fc1[task_index](hi)\n",
    "            hi = self.sigmoid(hi)\n",
    "            task_out[task_name] = hi    \n",
    "            \n",
    "        if len(self.task_name_list) == 1:\n",
    "            return task_out[self.task_name_list[0]]\n",
    "        else:\n",
    "            return task_out\n",
    "    \n",
    "    def predict_prob(self, x):\n",
    "        self.eval()\n",
    "        prob_dict = self.forward(x)\n",
    "        \n",
    "        if len(self.task_name_list) == 1:\n",
    "            prob_dict_true = {}\n",
    "            prob_dict_true[self.task_name_list[0]] = prob_dict\n",
    "            return prob_dict_true\n",
    "        return prob_dict\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        self.eval()\n",
    "        prob_dict = self.forward(x)\n",
    "        \n",
    "        if len(self.task_name_list) == 1:\n",
    "            prob_dict_true = {}\n",
    "            prob_dict_true[self.task_name_list[0]] = prob_dict\n",
    "            return prob_dict_true\n",
    "        \n",
    "        return prob_dict\n",
    "    \n",
    "    def predict(self, x, threshold = 0.5):\n",
    "        self.eval()\n",
    "        prob_dict = self.predict_prob(x)\n",
    "        pred_dict = {}\n",
    "        \n",
    "        for key, value in prob_dict.items():\n",
    "            #tensor轉numpy\n",
    "            value = value.cpu().detach().numpy()\n",
    "            pred_class = [1 if x > threshold else 0 for x in value]\n",
    "            pred_dict[key] = np.array(pred_class) \n",
    "        return pred_dict\n",
    "    \n",
    "    def evaluate(self,X,label,task_name,criterion):\n",
    "        with torch.no_grad():\n",
    "            prob = self.predict_prob(X)[task_name].cpu().detach().numpy() #tensor=>numpy\n",
    "            pred = self.predict(X)[task_name] \n",
    "            score = compute_scores(label,pred,prob)\n",
    "            score['task'] = task_name\n",
    "            loss = criterion(torch.from_numpy(prob).to(device),torch.from_numpy(label).to(device)).item()\n",
    "            score['loss'] = loss/len(label)\n",
    "            return score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d8537c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(y_true, y_pred,y_prob):\n",
    "    if np.any(np.isnan(y_prob)):\n",
    "        print(y_prob)\n",
    "        input()\n",
    "        \n",
    "    scores = {}\n",
    "    try:\n",
    "        scores['task'] = 'Null'\n",
    "        scores['auroc'] = round(roc_auc_score(y_true, y_prob), 3)\n",
    "        scores['acc'] = round(accuracy_score(y_true, y_pred), 3)\n",
    "        scores['f1'] = round(f1_score(y_true, y_pred), 3)\n",
    "        scores['pre'] = round(precision_score(y_true, y_pred), 3)\n",
    "        scores['recall'] = round(recall_score(y_true, y_pred), 3)\n",
    "        scores['brier_score'] = round(brier_score_loss(y_true, y_prob), 3)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "06f86098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader_dict, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    task_name_list = list(loader_dict.keys())\n",
    "\n",
    "    \"\"\"\n",
    "    cycle_loader\n",
    "    \"\"\"\n",
    "    max_length = 0\n",
    "    cycle_loader_dict = {}\n",
    "    for task_name, dataloader in loader_dict.items():\n",
    "        current_length = len(dataloader)\n",
    "        max_length = max(max_length, current_length)\n",
    "        cycle_loader_dict[task_name] = itertools.cycle(dataloader)\n",
    "\n",
    "    for i in range(max_length):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss = torch.zeros(1).to(device)\n",
    "        for task_name in task_name_list:  \n",
    "            data = next(cycle_loader_dict[task_name])\n",
    "            x = data[0].to(device)\n",
    "            label = data[1].unsqueeze(1).to(device)\n",
    "            prob = model.predict_prob(x)[task_name]\n",
    "            loss = criterion(prob, label)\n",
    "            total_loss += loss\n",
    "            if task_name == 'Weaning_successful':\n",
    "                total_loss += loss\n",
    "            \n",
    "        total_loss.backward()\n",
    "        train_loss+=total_loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        for task_name in task_name_list: \n",
    "            if (i + 1) % len(loader_dict[task_name]) == 0:\n",
    "                cycle_loader_dict[task_name] = itertools.cycle(loader_dict[task_name])\n",
    "                \n",
    "    train_loss /= max_length* 256 * len(task_name_list)\n",
    "    \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8daed213",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "    model\n",
    "    dict: Mydataset\n",
    "    loss_function\n",
    "Output:\n",
    "    score: dict + dict\n",
    "    result: dict => ['total_auc','total_loss']\n",
    "\"\"\"\n",
    "def test(model, dataset_dict, criterion, is_show = True , only_Weaning = False):\n",
    "    model.eval()\n",
    "\n",
    "    task_name_list = list(dataset_dict.keys())\n",
    "    score = {}\n",
    "    result = {'total_auc': 0, 'total_loss': 0}\n",
    "    for task_name in task_name_list:  # 循環每個任務\n",
    "        X = dataset_dict[task_name].inputs.numpy()\n",
    "        Y = dataset_dict[task_name].labels.unsqueeze(1).numpy()\n",
    "    \n",
    "        score[task_name] = model.evaluate(X,Y,task_name,criterion)\n",
    "        \n",
    "        if only_Weaning == True and 'Weaning_succecssful' in task_name_list:\n",
    "            if task_name == 'Weaning_succecssful':\n",
    "                result['total_auc'] = result['total_auc'] + score[task_name]['auroc']\n",
    "                result['total_loss'] = result['total_loss'] + score[task_name]['loss']\n",
    "        else:\n",
    "            result['total_auc'] = result['total_auc'] + score[task_name]['auroc']\n",
    "            result['total_loss'] = result['total_loss'] + score[task_name]['loss']\n",
    "            \n",
    "        if is_show:\n",
    "            print(score[task_name])\n",
    "    \n",
    "    return score,result\n",
    "\n",
    "\"\"\"\n",
    "local_best_model_dict: #dict{'task_name':{'model','performance(target_score)','id'}}\n",
    "model\n",
    "\"\"\"\n",
    "def test2(local_best_model_dict, modelr, dataset_dict, criterion, is_show = True):\n",
    "    score = {}\n",
    "    result = {'total_auc': 0, 'total_loss': 0}\n",
    "    task_name_list = list(dataset_dict.keys())\n",
    "    \n",
    "    for task_name in task_name_list:\n",
    "        print(f\"task: {task_name} \")\n",
    "        print(f\"{local_best_model_dict[task_name]['performance']}\")\n",
    "        modelr.load_state_dict(local_best_model_dict[task_name]['model'])\n",
    "        modelr.eval()\n",
    "        X = dataset_dict[task_name].inputs.numpy()\n",
    "        Y = dataset_dict[task_name].labels.unsqueeze(1).numpy()\n",
    "        score[task_name] = modelr.evaluate(X,Y,task_name,criterion)\n",
    "        result['total_auc'] = result['total_auc'] + score[task_name]['auroc']\n",
    "        result['total_loss'] = result['total_loss'] + score[task_name]['loss']\n",
    "        if is_show:\n",
    "            print(score[task_name])\n",
    "            \n",
    "    return score,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010a49fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2d074fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, np_X_scalar,np_X_original, np_Y):\n",
    "        self.inputs = torch.from_numpy(np_X_scalar).float()\n",
    "        self.inputs_original = torch.from_numpy(np_X_original).float()\n",
    "        self.labels = torch.from_numpy(np_Y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "    \n",
    "    def remove_samples(self, feature_index, threshold, condition_type):\n",
    "        \"\"\"\n",
    "        Remove samples based on a specified condition on a specific feature.\n",
    "\n",
    "        Parameters:\n",
    "        - feature_index (int): Index of the feature.\n",
    "        - threshold (float): Threshold value for the condition.\n",
    "        - condition_type (str): Type of condition ('type1' for '<' or 'type2' for '>=').\n",
    "        \"\"\"\n",
    "        if condition_type == 'type1':\n",
    "            indices_to_remove = torch.nonzero(self.inputs[:, feature_index] < threshold).squeeze()\n",
    "        elif condition_type == 'type2':\n",
    "            indices_to_remove = torch.nonzero(self.inputs[:, feature_index] >= threshold).squeeze()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid condition_type. Use 'type1' for '<' or 'type2' for '>='.\")\n",
    "\n",
    "        # Remove samples\n",
    "        self.inputs = torch.index_select(self.inputs, 0, indices_to_remove)\n",
    "        self.inputs_original = torch.index_select(self.inputs_original, 0, indices_to_remove)\n",
    "        self.labels = torch.index_select(self.labels, 0, indices_to_remove)\n",
    "    \n",
    "class BCEFocalLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=2, alpha=0.25, reduction='elementwise_mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    " \n",
    "    def forward(self, _input, target):\n",
    "        pt = _input\n",
    "        alpha = self.alpha\n",
    "        loss = - alpha * (1 - pt) ** self.gamma * target * torch.log(pt) - \\\n",
    "               (1 - alpha) * pt ** self.gamma * (1 - target) * torch.log(1 - pt)\n",
    "        if self.reduction == 'elementwise_mean':\n",
    "            loss = torch.mean(loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = torch.sum(loss)\n",
    "        return loss    \n",
    "\n",
    "    \n",
    "def check_label_distribution (data_Y):\n",
    "    count_1 = np.count_nonzero(data_Y == 1)\n",
    "    count_0 = np.count_nonzero(data_Y == 0)\n",
    "    count_others = np.count_nonzero((data_Y != 1) & (data_Y != 0))\n",
    "    ratio_1 = round(count_1/len(data_Y)*100,2)\n",
    "    ratio_0 = round(count_0/len(data_Y)*100,2)\n",
    "    ratio_others = round(count_others/len(data_Y)*100,2)\n",
    "    print(f'Distribution: 1=>{count_1}({ratio_1}%),  0=>{count_0}({ratio_0}%),  others=>{count_others}({ratio_others}%)')\n",
    "\n",
    "    \n",
    "def upsampling_auto(X,X_original,Y,up_ratio):\n",
    "    check_label_distribution(Y)\n",
    "    zero_idx = np.where(Y == 0)[0]\n",
    "    one_idx = np.where(Y == 1)[0]\n",
    "    other_idx = np.where((Y != 1) & (Y != 0))[0]\n",
    "    if len(other_idx > 0):\n",
    "        return X,Y\n",
    "    repeated_data_X = np.tile(X[one_idx], (up_ratio, 1, 1))\n",
    "    repeated_data_X_original = np.tile(X_original[one_idx], (up_ratio, 1, 1))\n",
    "    repeated_data_Y = np.tile(Y[one_idx], (up_ratio))\n",
    "\n",
    "    X_upsampled = np.vstack((X[zero_idx], repeated_data_X))\n",
    "    X_original_upsampled = np.vstack((X_original[zero_idx], repeated_data_X_original))\n",
    "\n",
    "    Y_upsampled = np.concatenate((Y[zero_idx], repeated_data_Y)) \n",
    "    return X_upsampled,X_original_upsampled, Y_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632dde32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e2ad79cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    X: numpy\n",
    "    feature_name_list : List\n",
    "    select_feature_list : List   (必須是feature_name_list的子集)\n",
    "Output\n",
    "    select_feature_list data\n",
    "\"\"\"\n",
    "def select_features(X, feature_name_list, select_feature_list):\n",
    "    invalid_features = set(select_feature_list) - set(feature_name_list)\n",
    "    if invalid_features:\n",
    "        raise ValueError(f\"Invalid features in select_feature_list: {invalid_features}\")\n",
    "    selected_feature_indices = [feature_name_list.index(feature) for feature in select_feature_list]\n",
    "    X_selected = X[:, :, selected_feature_indices]\n",
    "\n",
    "    return X_selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "958621ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    dataset_dict: Mydataset \n",
    "    loader_dict: Dataloader\n",
    "    feature_name_list: List\n",
    "    select_feature_list: List \n",
    "    batch_size: 256\n",
    "\n",
    "Output:\n",
    "    dataset_dict\n",
    "    loader_dict\n",
    "    feature_name_list ==>\n",
    "\"\"\"\n",
    "\n",
    "def read_data(task_name_list, data_date ,data_type, select_feature_list = [], batch_size = 256,use_upsample = False):\n",
    "    data_path = \"data/sample/standard_data\"\n",
    "    \n",
    "    #Feature name\n",
    "    df_feature = pd.read_csv(\"data/sample/full_feature_name.csv\")\n",
    "    feature_name_list = df_feature.columns.to_list()\n",
    "    \n",
    "    #Dataset\n",
    "    dataset_dict = {}\n",
    "    for task_name in task_name_list:\n",
    "        X_scalar = np.load(f\"{data_path}/{data_type}_scalar_X_{task_name}.npy\", allow_pickle=True)\n",
    "        X_original = np.load(f\"{data_path}/{data_type}_X_{task_name}.npy\", allow_pickle=True)\n",
    "        \n",
    "        if len(select_feature_list)>0:\n",
    "            X_scalar = select_features(X_scalar,feature_name_list,select_feature_list)\n",
    "            X_original = select_features(X_original,feature_name_list,select_feature_list)\n",
    "            assert X_scalar.shape[2] == len(select_feature_list)\n",
    "            assert X_original.shape[2] == len(select_feature_list)\n",
    "            \n",
    "        Y = np.load(f\"{data_path}\\\\{data_type}_Y_{task_name}.npy\", allow_pickle=True)\n",
    "        \n",
    "        if use_upsample:\n",
    "            if task_name == 'Weaning_successful' and data_type != 'test':\n",
    "                X_scalar,X_original,Y = upsampling_auto(X_scalar,X_original,Y,2)\n",
    "        dataset_dict[task_name] = MyDataset(X_scalar,X_original,Y)\n",
    "    \n",
    "    #Dataloader\n",
    "    loader_dict = {}\n",
    "    for key, dataset in dataset_dict.items():        \n",
    "        loader_dict[key] = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return dataset_dict,loader_dict,feature_name_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "70496de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MTL_to_STL(multi_task_model):\n",
    "    single_task_models = {}\n",
    "\n",
    "    for task_index, task_name in enumerate(multi_task_model.task_name_list):\n",
    "        \n",
    "        single_task_model = MLP_MTL(input_dim, [task_name])  \n",
    "        single_task_model.to(device) \n",
    "\n",
    "        single_task_model.bt_fc1.weight.data = multi_task_model.bt_fc1.weight.data.clone()\n",
    "        single_task_model.bt_fc1.bias.data = multi_task_model.bt_fc1.bias.data.clone()\n",
    "\n",
    "        single_task_model.bt_fc2.weight.data = multi_task_model.bt_fc2.weight.data.clone()\n",
    "        single_task_model.bt_fc2.bias.data = multi_task_model.bt_fc2.bias.data.clone()\n",
    "\n",
    "        single_task_model.bt_fc3.weight.data = multi_task_model.bt_fc3.weight.data.clone()\n",
    "        single_task_model.bt_fc3.bias.data = multi_task_model.bt_fc3.bias.data.clone()\n",
    "\n",
    "        single_task_model.task_fc0[0].weight.data = multi_task_model.task_fc0[task_index].weight.data.clone()\n",
    "        single_task_model.task_fc0[0].bias.data = multi_task_model.task_fc0[task_index].bias.data.clone()\n",
    "\n",
    "        single_task_model.task_fc1[0].weight.data = multi_task_model.task_fc1[task_index].weight.data.clone()\n",
    "        single_task_model.task_fc1[0].bias.data = multi_task_model.task_fc1[task_index].bias.data.clone()\n",
    "\n",
    "        single_task_models[task_name] = single_task_model\n",
    "    return single_task_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f4dc4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(task_name_list, model_parm_dict, mode, time_id):\n",
    "\n",
    "    path = ''\n",
    "    if len(task_name_list) == 1:\n",
    "        path = \"./model/group_result/stl_group\"\n",
    "    elif 'SBT_start' in task_name_list:\n",
    "        path = \"./model/group_result/mtl_group/vent_group\"\n",
    "    else:\n",
    "        path = \"./model/group_result/mtl_group/mortality_group\"\n",
    "    \n",
    "    for task_name in task_name_list:\n",
    "        model_parm = model_parm_dict[task_name].state_dict().copy()\n",
    "\n",
    "        if mode == 'lite':\n",
    "            torch.save(model_parm, f'{path}/{task_name}_{time_id}_lite')\n",
    "        else:\n",
    "            torch.save(model_parm, f'{path}/{task_name}_{time_id}')\n",
    "\n",
    "def save_feature_name(feature_name_list,path,file_name):\n",
    "    df_feature = pd.DataFrame()\n",
    "    df_feature['Feature'] = feature_name_list\n",
    "    df_feature.to_csv(f'{path}/{file_name}.csv',index = False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9a371a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "    experiment_time\n",
    "    max_epoch\n",
    "    learning_rate\n",
    "    input_dim\n",
    "    task_name_list\n",
    "    train_loader_dict\n",
    "    val_dataset_dict\n",
    "    test_dataset_dict\n",
    "    device\n",
    "    is_show\n",
    "\n",
    "Output:\n",
    "    df_grade\n",
    "    stl_model_dict\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def train_and_test_model(experiment_time, max_epoch, learning_rate, input_dim, task_name_list, train_loader_dict, val_dataset_dict, test_dataset_dict, device,is_show = True , load_parm = {}):\n",
    "    df_grade = pd.DataFrame(columns=['time', 'task', 'auroc', 'acc', 'f1', 'pre', 'recall', 'brier_score', 'loss'])\n",
    "    best_model_params = {}\n",
    "    global_best_AUC = 0\n",
    "    global_best_loss = 10000\n",
    "    best_model_dict = {} \n",
    "    \n",
    "    count = 1\n",
    "    local_indicator = 'auroc'\n",
    "    global_indicator = 'loss'\n",
    "    \n",
    "    if len(load_parm)!=0:\n",
    "        assert len(task_name_list) == 1 or 'Weaning_successful' not in task_name_list, f'The number of tasks is not 1 => {task_name_list}'\n",
    "        \n",
    "        load_model = MLP_MTL(input_dim, task_name_list).to(device)\n",
    "        load_model.load_state_dict(load_parm)\n",
    "        load_model.eval()\n",
    "        \n",
    "        loss_func = BCEFocalLoss(alpha=alpha, gamma=gamma)\n",
    "        \n",
    "        result = test(load_model, test_dataset_dict, loss_func, is_show = True)\n",
    "        return load_model,result\n",
    "    \n",
    "    for time in range(experiment_time):\n",
    "        train_loss_list = []\n",
    "        val_loss_list = []\n",
    "        auc_list = []    \n",
    "        local_best_AUC = 0\n",
    "        local_best_loss = 10000\n",
    "        local_best_model_dict = {} \n",
    "        patience_counter = 0\n",
    "        \n",
    "        model = MLP_MTL(input_dim, task_name_list).to(device)\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "        loss_func = BCEFocalLoss(alpha=alpha, gamma=gamma)\n",
    "        \n",
    "        for epoch in tqdm(range(max_epoch)):\n",
    "            if is_show:\n",
    "                print(f'Time:{time+1}/{experiment_time} - Epoch:{epoch+1}/{max_epoch}...')\n",
    "                \n",
    "            train_loss = train(model, train_loader_dict, loss_func, optimizer)\n",
    "            val_score_dict, result = test(model, val_dataset_dict, loss_func, is_show= is_show, only_Weaning = only_Weaning)\n",
    "            \n",
    "            train_loss_list.append(train_loss)\n",
    "            val_loss_list.append(result['total_loss'])\n",
    "            auc_list.append(result['total_auc'])\n",
    "            \n",
    "            ########################################################################################################################\n",
    "            for task_name in task_name_list:\n",
    "                if task_name not in local_best_model_dict:\n",
    "                    model_dict = {}\n",
    "                    model_dict['model'] = model.state_dict().copy()\n",
    "                    model_dict['performance'] = val_score_dict[task_name]\n",
    "                    model_dict['id'] = count\n",
    "                    local_best_model_dict[task_name] = model_dict\n",
    "                else:\n",
    "                    target_score = val_score_dict[task_name]\n",
    "                    if local_indicator == 'auroc':\n",
    "                        if local_best_model_dict[task_name]['performance'][local_indicator] < target_score[local_indicator] :\n",
    "                            local_best_model_dict[task_name]['performance'] = target_score\n",
    "                            local_best_model_dict[task_name]['model'] = model.state_dict().copy()\n",
    "                            local_best_model_dict[task_name]['id'] = count                           \n",
    "                    else:\n",
    "                        if local_best_model_dict[task_name]['performance'][local_indicator] > target_score[local_indicator] :\n",
    "                            local_best_model_dict[task_name]['performance'] = target_score\n",
    "                            local_best_model_dict[task_name]['model'] = model.state_dict().copy()\n",
    "                            local_best_model_dict[task_name]['id'] = count\n",
    "                    \n",
    "            for task_name in task_name_list:\n",
    "                if task_name not in best_model_dict:\n",
    "                    model_dict = {}\n",
    "                    model_dict['model'] = model.state_dict().copy()\n",
    "                    model_dict['performance'] = val_score_dict[task_name]\n",
    "                    model_dict['id'] = count\n",
    "                    best_model_dict[task_name] = model_dict\n",
    "                else:\n",
    "                    target_score = val_score_dict[task_name]\n",
    "                    if local_indicator == 'auroc':\n",
    "                        if best_model_dict[task_name]['performance'][local_indicator] < target_score[local_indicator] :\n",
    "                            best_model_dict[task_name]['performance'] = target_score\n",
    "                            best_model_dict[task_name]['model'] = model.state_dict().copy()\n",
    "                            best_model_dict[task_name]['id'] = count\n",
    "                    else:\n",
    "                        if best_model_dict[task_name]['performance'][local_indicator] > target_score[local_indicator] :\n",
    "                            best_model_dict[task_name]['performance'] = target_score\n",
    "                            best_model_dict[task_name]['model'] = model.state_dict().copy()\n",
    "                            best_model_dict[task_name]['id'] = count\n",
    "            count+=1\n",
    "\n",
    "            ########################################################################################################################\n",
    "            \"\"\" Early stop \"\"\"\n",
    "            if global_indicator == 'loss':\n",
    "                if result['total_loss'] < local_best_loss:\n",
    "                    local_best_loss = result['total_loss']\n",
    "                    if local_best_loss < global_best_loss:\n",
    "                        global_best_loss = local_best_loss\n",
    "                        best_model_params = model.state_dict().copy() \n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1     \n",
    "            else:\n",
    "                if result['total_auc'] > local_best_AUC:\n",
    "                    local_best_AUC = result['total_auc']\n",
    "                    if local_best_AUC > global_best_AUC:\n",
    "                        global_best_AUC = local_best_AUC\n",
    "                        best_model_params = model.state_dict().copy()\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1 \n",
    "            \n",
    "            global limit_early_stop_count\n",
    "            if patience_counter >= limit_early_stop_count:\n",
    "                break\n",
    "        \n",
    "        #is_show = True\n",
    "        empty_model = MLP_MTL(input_dim, task_name_list).to(device)\n",
    "        test_score_dict, result = test2(local_best_model_dict, empty_model, test_dataset_dict, loss_func, is_show = is_show)\n",
    "        ########################################################################################################################\n",
    "        #input()\n",
    "        for task_name in task_name_list:\n",
    "            test_score_dict[task_name]['time'] = time + 1\n",
    "            df_grade = pd.concat([df_grade, pd.DataFrame.from_records([test_score_dict[task_name]])])\n",
    "        print(df_grade)\n",
    "        print('----------finished----------')\n",
    "    \n",
    "    \n",
    "    global_stl_model_dict = {}\n",
    "    for task_name in task_name_list:\n",
    "        model = MLP_MTL(input_dim, task_name_list).to(device)\n",
    "        model.load_state_dict(best_model_dict[task_name]['model'])\n",
    "        local_stl_model_dict = MTL_to_STL(model)\n",
    "        global_stl_model_dict[task_name] = local_stl_model_dict[task_name]\n",
    "    \n",
    "    return df_grade, global_stl_model_dict, best_model_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "be71fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "Input:\n",
    "    shap_value: flatten的結果,[sample,feature]\n",
    "    feature_name_list: flatten的feature list\n",
    "Output:\n",
    "    feature_important \n",
    "    sum_per_feature \n",
    "\"\"\"\n",
    "def calculate_feature_important(shap_value,feature_name_list):\n",
    "    abs_shap_value = np.abs(shap_value)\n",
    "    sum_per_feature = np.sum(abs_shap_value, axis=0)\n",
    "    sorted_feature_indices = np.argsort(sum_per_feature)[::-1] #[::-1]是reversed\n",
    "    sorted_feature_names = [feature_name_list[i] for i in sorted_feature_indices]\n",
    "    return sorted_feature_names, sum_per_feature\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    model  \n",
    "    train_X (np)\n",
    "    test_X  (np)\n",
    "    test_X_original (np)\n",
    "    feature_name_list (list)\n",
    "    task_name (string)\n",
    "Output:\n",
    "    shap_value\n",
    "    shap_data\n",
    "    (flatten的結果)\n",
    "\"\"\"\n",
    "def get_model_shap(model,data_X_train,data_X_test,data_X_test_original,feature_name_list,task_name,use_mini_sample = True,n_sample = 100):\n",
    "    \n",
    "    max_sample = 1000\n",
    "    seq_day = data_X_train.shape[1]\n",
    "    feature_count = data_X_train.shape[2]\n",
    "    \n",
    "    if use_mini_sample:\n",
    "        background_data = torch.from_numpy(data_X_train[:max_sample]).float().to(device)\n",
    "        shap_data = torch.from_numpy(data_X_test[:max_sample]).float().to(device)\n",
    "        shap_data_original = torch.from_numpy(data_X_test_original[:max_sample]).float().to(device)\n",
    "    else:\n",
    "        background_data = torch.from_numpy(data_X_train[:]).float().to(device)\n",
    "        shap_data = torch.from_numpy(data_X_test[:]).float().to(device)\n",
    "        shap_data_original = torch.from_numpy(data_X_test_original[:]).float().to(device)\n",
    "\n",
    "    model.eval()\n",
    "    explainer = shap.GradientExplainer(model, background_data)\n",
    "    \n",
    "    shap_values = explainer.shap_values(shap_data,nsamples=n_sample)\n",
    "    shap_values = np.array(shap_values)\n",
    "    \n",
    "    shap_value_flatten = np.zeros((len(shap_data),seq_day*feature_count))\n",
    "    shap_data_flatten = np.zeros((len(shap_data),seq_day*feature_count))\n",
    "    \n",
    "    for i in range(0,len(shap_data)):\n",
    "        count=0\n",
    "        for j in range(feature_count):\n",
    "            for k in range(seq_day):\n",
    "                shap_value_flatten[i][count]=shap_values[i][k][j]  \n",
    "                shap_data_flatten[i][count]=shap_data_original[i][k][j]  \n",
    "                count += 1\n",
    "    feature_important,_ = calculate_feature_important(shap_value_flatten, feature_name_list)\n",
    "    return feature_important, shap_value_flatten, shap_data_flatten\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    shap_value_flatten (sample,feature_flatten)\n",
    "    shap_data_flatten (sample,feature_flatten)\n",
    "    max_display \n",
    "\"\"\"\n",
    "def show_shap(shap_value_flatten, shap_data_flatten,feature_name_list, max_display = 20,task_name = ''):\n",
    "    fig = shap.summary_plot(shap_value_flatten,shap_data_flatten,feature_names=feature_name_list, show=False,max_display = max_display)\n",
    "    plt.title(f\"***Task:{task_name}***\")\n",
    "    ax = plt.gca()  \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bff3ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e31150f",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f8a8e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_feature_list = []\n",
    "\n",
    "if use_mini_feature:\n",
    "    df_select_feature = pd.read_csv('./data/sample/select_feature.csv')\n",
    "    select_feature_list = df_select_feature['feature'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "041cbcbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> input_dim: 18\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  1%|▊                                                                                 | 1/100 [00:01<02:49,  1.71s/it]Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      " 47%|██████████████████████████████████████                                           | 47/100 [01:05<01:13,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task: Weaning_successful \n",
      "{'task': 'Weaning_successful', 'auroc': 0.766, 'acc': 0.736, 'f1': 0.523, 'pre': 0.565, 'recall': 0.486, 'brier_score': 0.178, 'loss': 0.00035952110257413654}\n",
      "task: SBT_start \n",
      "{'task': 'SBT_start', 'auroc': 0.817, 'acc': 0.751, 'f1': 0.642, 'pre': 0.724, 'recall': 0.577, 'brier_score': 0.175, 'loss': 0.000246719583957153}\n",
      "task: SBT_successful \n",
      "{'task': 'SBT_successful', 'auroc': 0.799, 'acc': 0.74, 'f1': 0.574, 'pre': 0.702, 'recall': 0.486, 'brier_score': 0.177, 'loss': 0.00025600404806540045}\n",
      "  time                task  auroc    acc     f1    pre  recall  brier_score  \\\n",
      "0    1  Weaning_successful  0.855  0.792  0.586  0.688   0.510        0.153   \n",
      "0    1           SBT_start  0.831  0.732  0.616  0.757   0.519        0.175   \n",
      "0    1      SBT_successful  0.825  0.739  0.583  0.758   0.474        0.174   \n",
      "\n",
      "       loss  \n",
      "0  0.000300  \n",
      "0  0.000249  \n",
      "0  0.000255  \n",
      "----------finished----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  1%|▊                                                                                 | 1/100 [00:01<02:17,  1.39s/it]Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  2%|█▋                                                                                | 2/100 [00:02<02:10,  1.34s/it]Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      " 38%|██████████████████████████████▊                                                  | 38/100 [00:52<01:26,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task: Weaning_successful \n",
      "{'task': 'Weaning_successful', 'auroc': 0.765, 'acc': 0.756, 'f1': 0.542, 'pre': 0.612, 'recall': 0.486, 'brier_score': 0.175, 'loss': 0.0003602688925133811}\n",
      "task: SBT_start \n",
      "{'task': 'SBT_start', 'auroc': 0.815, 'acc': 0.74, 'f1': 0.605, 'pre': 0.732, 'recall': 0.515, 'brier_score': 0.173, 'loss': 0.0002481326666574271}\n",
      "task: SBT_successful \n",
      "{'task': 'SBT_successful', 'auroc': 0.796, 'acc': 0.738, 'f1': 0.558, 'pre': 0.713, 'recall': 0.458, 'brier_score': 0.175, 'loss': 0.0002593320740060787}\n",
      "  time                task  auroc    acc     f1    pre  recall  brier_score  \\\n",
      "0    1  Weaning_successful  0.855  0.792  0.586  0.688   0.510        0.153   \n",
      "0    1           SBT_start  0.831  0.732  0.616  0.757   0.519        0.175   \n",
      "0    1      SBT_successful  0.825  0.739  0.583  0.758   0.474        0.174   \n",
      "0    2  Weaning_successful  0.840  0.792  0.615  0.659   0.577        0.158   \n",
      "0    2           SBT_start  0.830  0.732  0.616  0.757   0.519        0.176   \n",
      "0    2      SBT_successful  0.820  0.731  0.573  0.738   0.469        0.176   \n",
      "\n",
      "       loss  \n",
      "0  0.000300  \n",
      "0  0.000249  \n",
      "0  0.000255  \n",
      "0  0.000311  \n",
      "0  0.000253  \n",
      "0  0.000261  \n",
      "----------finished----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  1%|▊                                                                                 | 1/100 [00:01<02:03,  1.25s/it]Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  2%|█▋                                                                                | 2/100 [00:02<02:04,  1.28s/it]Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  3%|██▍                                                                               | 3/100 [00:03<02:06,  1.30s/it]Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      " 51%|█████████████████████████████████████████▎                                       | 51/100 [01:09<01:06,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task: Weaning_successful \n",
      "{'task': 'Weaning_successful', 'auroc': 0.771, 'acc': 0.75, 'f1': 0.526, 'pre': 0.602, 'recall': 0.467, 'brier_score': 0.175, 'loss': 0.00035499483346939086}\n",
      "task: SBT_start \n",
      "{'task': 'SBT_start', 'auroc': 0.807, 'acc': 0.748, 'f1': 0.624, 'pre': 0.736, 'recall': 0.541, 'brier_score': 0.18, 'loss': 0.0002511007898657985}\n",
      "task: SBT_successful \n",
      "{'task': 'SBT_successful', 'auroc': 0.792, 'acc': 0.736, 'f1': 0.597, 'pre': 0.664, 'recall': 0.542, 'brier_score': 0.179, 'loss': 0.0002605913270647137}\n",
      "  time                task  auroc    acc     f1    pre  recall  brier_score  \\\n",
      "0    1  Weaning_successful  0.855  0.792  0.586  0.688   0.510        0.153   \n",
      "0    1           SBT_start  0.831  0.732  0.616  0.757   0.519        0.175   \n",
      "0    1      SBT_successful  0.825  0.739  0.583  0.758   0.474        0.174   \n",
      "0    2  Weaning_successful  0.840  0.792  0.615  0.659   0.577        0.158   \n",
      "0    2           SBT_start  0.830  0.732  0.616  0.757   0.519        0.176   \n",
      "0    2      SBT_successful  0.820  0.731  0.573  0.738   0.469        0.176   \n",
      "0    3  Weaning_successful  0.847  0.801  0.650  0.657   0.644        0.160   \n",
      "0    3           SBT_start  0.826  0.732  0.638  0.723   0.571        0.177   \n",
      "0    3      SBT_successful  0.814  0.745  0.616  0.734   0.531        0.177   \n",
      "\n",
      "       loss  \n",
      "0  0.000300  \n",
      "0  0.000249  \n",
      "0  0.000255  \n",
      "0  0.000311  \n",
      "0  0.000253  \n",
      "0  0.000261  \n",
      "0  0.000311  \n",
      "0  0.000252  \n",
      "0  0.000260  \n",
      "----------finished----------\n"
     ]
    }
   ],
   "source": [
    "#select_feature_list = []\n",
    "top_percent = 0.9\n",
    "remove_time_count = 0\n",
    "full_result_dict = {}\n",
    "featurn_count_list = []\n",
    "\n",
    "\n",
    "while len(select_feature_list)==0 or len(select_feature_list) > 5:\n",
    "    ########################################################################################################################\n",
    "    remove_time_count += 1\n",
    "    \"\"\"\n",
    "    read data\n",
    "    \"\"\"\n",
    "    train_dataset_dict,train_loader_dict,feature_name_list = read_data(task_name_list,'','train',select_feature_list,batch_size = batch_size,use_upsample = use_upsample)\n",
    "    val_dataset_dict,val_loader_dict,_ = read_data(task_name_list,'','validation',select_feature_list,batch_size = batch_size,use_upsample = use_upsample)\n",
    "    test_dataset_dict,test_loader_dict,_ = read_data(task_name_list,'','test',select_feature_list,batch_size = batch_size,use_upsample = use_upsample)\n",
    "    if len(select_feature_list)!=0:\n",
    "        feature_name_list = select_feature_list\n",
    "    input_dim = train_dataset_dict[task_name_list[0]].inputs.numpy().shape[2]\n",
    "    print(f'==> input_dim: {input_dim}')\n",
    "    featurn_count_list.append(input_dim)\n",
    "    ########################################################################################################################\n",
    "    \"\"\"\n",
    "    Train\n",
    "    \"\"\"\n",
    "    print(\"training\")\n",
    "    df_grade, stl_model_dict, best_model_dict = train_and_test_model(experiment_time = experiment_time, \n",
    "                                                     max_epoch = max_epoch, \n",
    "                                                     learning_rate = learning_rate, \n",
    "                                                     input_dim = input_dim, \n",
    "                                                     task_name_list = task_name_list, \n",
    "                                                     train_loader_dict = train_loader_dict, \n",
    "                                                     val_dataset_dict = val_dataset_dict, \n",
    "                                                     test_dataset_dict = test_dataset_dict, \n",
    "                                                     device = device, is_show = False)\n",
    "    df_grade['remove_time'] = remove_time_count\n",
    "    full_result_dict[remove_time_count] = {}\n",
    "    full_result_dict[remove_time_count]['result'] = df_grade\n",
    "    full_result_dict[remove_time_count]['model'] = stl_model_dict\n",
    "    full_result_dict[remove_time_count]['select_feature_list'] = feature_name_list\n",
    "    \n",
    "    if calculate_shap == False:\n",
    "        break\n",
    "        \n",
    "    ########################################################################################################################\n",
    "    \"\"\"\n",
    "    Shap\n",
    "    \"\"\"\n",
    "    shap_dict = {}\n",
    "    seq_day = 1\n",
    "    feature_count = len(feature_name_list)\n",
    "    sum_shap_value = np.zeros((0, seq_day * feature_count))\n",
    "    sum_shap_data = np.zeros((0, seq_day * feature_count))\n",
    "    #計算各任務的feature_important\n",
    "    for task_name in task_name_list:\n",
    "        shap_dict[task_name] = {}\n",
    "        feature_important, shap_value_flatten, shap_data_flatten = get_model_shap(\n",
    "                                                                    stl_model_dict[task_name],\n",
    "                                                                    train_dataset_dict[task_name].inputs.numpy(),\n",
    "                                                                    test_dataset_dict[task_name].inputs.numpy(),\n",
    "                                                                    test_dataset_dict[task_name].inputs_original.numpy(),\n",
    "                                                                    feature_name_list,\n",
    "                                                                    task_name,\n",
    "                                                                    use_mini_sample = False,\n",
    "                                                                    n_sample = 1)\n",
    "        \n",
    "        shap_dict[task_name]['feature_important'] = feature_important\n",
    "        shap_dict[task_name]['shap_value'] = shap_value_flatten\n",
    "        shap_dict[task_name]['shap_data'] = shap_data_flatten\n",
    "        shap_dict[task_name]['feature_name_list'] = feature_name_list\n",
    "\n",
    "        sum_shap_value = np.vstack([sum_shap_value, shap_value_flatten])\n",
    "        sum_shap_data = np.vstack([sum_shap_data, shap_data_flatten]) \n",
    "\n",
    "    full_result_dict[remove_time_count]['shap_result'] = shap_dict\n",
    "    \n",
    "    ########################################################################################################################\n",
    "    \"\"\"\n",
    "    select feature\n",
    "    \"\"\"\n",
    "    global_feature_important, _ = calculate_feature_important(sum_shap_value, feature_name_list)\n",
    "    full_result_dict[remove_time_count]['global_feature_important'] = global_feature_important\n",
    "    \n",
    "    if select_feature_flag == False:\n",
    "        break\n",
    "        \n",
    "    #print(global_feature_important)\n",
    "    shap.summary_plot(sum_shap_value,sum_shap_data,plot_type=\"bar\",feature_names=feature_name_list, show=False,max_display = 20)\n",
    "    #input()\n",
    "    if len(global_feature_important) <= 20:\n",
    "        select_feature_list = global_feature_important[:len(global_feature_important)-1]\n",
    "    else:\n",
    "        num_selected_features = int(len(global_feature_important) * top_percent)\n",
    "        num_selected_features = max(1, num_selected_features)\n",
    "        select_feature_list = global_feature_important[:num_selected_features]\n",
    "        \n",
    "    print(f'input dim:{len(global_feature_important)} ==> {len(select_feature_list)}.....')\n",
    "\n",
    "    ########################################################################################################################\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ebec25e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# +++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ce9a4982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save?(y/n)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\n",
    "max_length = 120\n",
    "if select_feature_flag:\n",
    "    df_select_features = pd.DataFrame()\n",
    "    path = \"./select_feature_result/\"\n",
    "    for time in range(1,remove_time_count+1):\n",
    "        \n",
    "        data_list = full_result_dict[time]['select_feature_list']\n",
    "        while len(data_list) < max_length:\n",
    "            data_list.append(\"\")\n",
    "            \n",
    "        df_select_features[f'time{time}'] = data_list\n",
    "        \n",
    "        for task_name in task_name_list:\n",
    "            model_parm = full_result_dict[time]['model'][task_name].state_dict()\n",
    "            torch.save(model_parm, f'{path}/{task_name}_{time}')\n",
    "            \n",
    "    df_select_features.to_csv(f\"{path}/select process.csv\",index = False)\n",
    "    print(\"儲存成功\")\n",
    "else:\n",
    "\n",
    "    if input(f'save?(y/n)') == 'y':\n",
    "\n",
    "        path = ''\n",
    "        if len(task_name_list) == 1:\n",
    "            path = \"./model/group_result/stl_group\"\n",
    "        elif 'SBT_start' in task_name_list:\n",
    "            path = \"./model/group_result/mtl_group/vent_group\"\n",
    "        else:\n",
    "            path = \"./model/group_result/mtl_group/mortality_group\"\n",
    "\n",
    "    \n",
    "        time = 1\n",
    "        save_feature_name(full_result_dict[time]['select_feature_list'], path, 'feature_name_list')\n",
    "        \n",
    "        for task_name in task_name_list:\n",
    "            model_parm = full_result_dict[time]['model'][task_name].state_dict()\n",
    "            if len(full_result_dict[time]['select_feature_list']) < 30:\n",
    "                torch.save(model_parm, f'{path}/{task_name}_best_lite')\n",
    "            else:\n",
    "                torch.save(model_parm, f'{path}/{task_name}_best')\n",
    "        print(\"finished\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "491b737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from datetime import datetime\n",
    "\n",
    "def group_result(df):\n",
    "    agg_columns = {\n",
    "        'acc': ['mean', 'std'],\n",
    "        'pre': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'auroc': ['mean', 'std'],\n",
    "        'brier_score': ['mean', 'std']\n",
    "    }\n",
    "    df_group = df.groupby('task').agg(agg_columns)\n",
    "    df_group.columns = [f\"{col[0]}_{col[1]}\" for col in df_group.columns]\n",
    "    for metric in ['acc', 'pre', 'f1', 'recall', 'auroc','brier_score']:\n",
    "        df_group[f\"{metric}_combined\"] = df_group.apply(\n",
    "            lambda row: f\"{row[f'{metric}_mean']:.4f} ± {row[f'{metric}_std']:.4f}\", axis=1\n",
    "        )\n",
    "    df_result = df_group[[f\"{metric}_combined\" for metric in ['acc', 'pre', 'f1', 'recall', 'auroc','brier_score']]]\n",
    "    df_result.reset_index(inplace=True)\n",
    "    df_result.columns = ['task','acc', 'pre', 'f1', 'recall', 'auroc','brier_score']\n",
    "    return df_result\n",
    "\n",
    "\n",
    "def save_to_xlsx(df_save,file_name = 'output'):\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    for r_idx, row in enumerate(dataframe_to_rows(df_save, index=False, header=True), 1):\n",
    "        for c_idx, value in enumerate(row, 1):\n",
    "            ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "    wb.save(f'{file_name}.xlsx')\n",
    "\n",
    "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "df_grade_group = group_result(df_grade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3b205ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>acc</th>\n",
       "      <th>pre</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "      <th>auroc</th>\n",
       "      <th>brier_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SBT_start</td>\n",
       "      <td>0.7320 ± 0.0000</td>\n",
       "      <td>0.7457 ± 0.0196</td>\n",
       "      <td>0.6233 ± 0.0127</td>\n",
       "      <td>0.5363 ± 0.0300</td>\n",
       "      <td>0.8290 ± 0.0026</td>\n",
       "      <td>0.1760 ± 0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SBT_successful</td>\n",
       "      <td>0.7383 ± 0.0070</td>\n",
       "      <td>0.7433 ± 0.0129</td>\n",
       "      <td>0.5907 ± 0.0225</td>\n",
       "      <td>0.4913 ± 0.0344</td>\n",
       "      <td>0.8197 ± 0.0055</td>\n",
       "      <td>0.1757 ± 0.0015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Weaning_successful</td>\n",
       "      <td>0.7950 ± 0.0052</td>\n",
       "      <td>0.6680 ± 0.0173</td>\n",
       "      <td>0.6170 ± 0.0320</td>\n",
       "      <td>0.5770 ± 0.0670</td>\n",
       "      <td>0.8473 ± 0.0075</td>\n",
       "      <td>0.1570 ± 0.0036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 task              acc              pre               f1  \\\n",
       "0           SBT_start  0.7320 ± 0.0000  0.7457 ± 0.0196  0.6233 ± 0.0127   \n",
       "1      SBT_successful  0.7383 ± 0.0070  0.7433 ± 0.0129  0.5907 ± 0.0225   \n",
       "2  Weaning_successful  0.7950 ± 0.0052  0.6680 ± 0.0173  0.6170 ± 0.0320   \n",
       "\n",
       "            recall            auroc      brier_score  \n",
       "0  0.5363 ± 0.0300  0.8290 ± 0.0026  0.1760 ± 0.0010  \n",
       "1  0.4913 ± 0.0344  0.8197 ± 0.0055  0.1757 ± 0.0015  \n",
       "2  0.5770 ± 0.0670  0.8473 ± 0.0075  0.1570 ± 0.0036  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grade_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48577937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
